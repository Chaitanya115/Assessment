{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing all the dependent libraries, Pandas will be majorly used library\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import glob\n",
    "import pymysql # mysql library\n",
    "\n",
    "\n",
    "# scripting is done on sample data, will share the sample data\n",
    "#df_file = pd.read_csv('/Users/chaitanyapandey/Documents/Aqilliz/Dataset.csv')\n",
    "\n",
    "def main():\n",
    "    \n",
    "''' \n",
    "Task 1: Data extraction\n",
    "Importing the csv data from s3 bucket using Boto3 library into python pandas dataframe\n",
    "(as pandas is highly optimised for structured data, to import data AWS access key and secret access key need to be provided\n",
    "sample data:\n",
    "Course_name\n",
    "11111111_An Introduction to American Law (University of Pennsylvania)-201902.csv\n",
    "12345678_Business Analysis (penn)-201902.csv\n",
    "12345676M_Foundation to Data Engineering (University of California)-201902.csv\n",
    "12343456_Foundation to Data Engineering (UC)-201902.csv\n",
    "123454321_Bachelor of Arts (Nanyang Technological University)-201902.csv\n",
    "12332112_Foundation of Medicine (Nanyang Technological University Singapore)-201902.csv\n",
    "12346789_Foundation of politics (NTU)-201902.csv\n",
    "12332112_Intermdiate of Blockchain (NTU)-201902.csv\n",
    "'''\n",
    "    REGION = 'Mention the Region'\n",
    "    ACCESS_KEY_ID = 'key id'\n",
    "    SECRET_ACCESS_KEY = 'Secret access key'\n",
    "\n",
    "    BUCKET_NAME = 'arn:aws:s3:::aqilliz-data-engineer-assessment'\n",
    "    KEY = '/*.csv' # file path in S3 \n",
    "\n",
    "    s3c = boto3.client(\n",
    "        's3', \n",
    "        region_name = REGION,\n",
    "        aws_access_key_id = ACCESS_KEY_ID,\n",
    "        aws_secret_access_key = SECRET_ACCESS_KEY\n",
    "      )\n",
    "    obj = s3c.get_object(Bucket= BUCKET_NAME , Key = KEY)\n",
    "    df_file = pd.read_csv(io.BytesIO(obj['Body'].read()), encoding='utf8')\n",
    "\n",
    "\n",
    "'''\n",
    "Task2 : Data Transformation\n",
    "\n",
    "once dataframe is created, now its need to be splitted into separate columns , so that further transformation\n",
    "rules can be applied.\n",
    "Assumption is input data format will be below always:\n",
    "e.g input_data = '11111111_An Introduction to American Law (University of Pennsylvania)-201902.csv'\n",
    "\n",
    "courseid and coursename delimitor is '_' (11111111_An Introduction to American Law)\n",
    "CourseName and UniversityName delimitor is '(' (An Introduction to American Law (University of Pennsylvania)\n",
    "UniversityName and YYYYMM delimitor is ')-' University of Pennsylvania)-201902\n",
    "'''\n",
    "    split = df_file.Course_name.str.split(r\"[_()-.]\",expand=True) # Splitting the dataframe\n",
    "\n",
    "    df_file['CourseID'] = split[0] # Courseid will be index 0\n",
    "    df_file['CourseName'] = split[1] # CourseName will be index 1\n",
    "    df_file['UniversityName'] = split[2] # UniversityName will be index 2\n",
    "    df_file['Year_mnth'] = split[4] # Year_mnth will be index 4 and csv will be index 5\n",
    "\n",
    "\n",
    "# As metioned in problem statement , university name - NTU contain Singapore as suffix in some of the records, \n",
    "#so trimmed the singapore keyword, if more university name contain more suffix, so approach is to do profiling \n",
    "#and identify all the pattern and make reference data to replace with '' in bulk\n",
    "    df_file['UniversityName'] = df_file['UniversityName'].replace(regex=r'.Singapore$', value='')\n",
    "\n",
    "'''\n",
    " To handle below situation, created university dataframe and later will use this dataframe to do lookup & return\n",
    " \n",
    "UC and University of California\n",
    "Pennsylvania and Penn\n",
    "Nanyang Technological University (some with trailing Singapore, some without) and NTU\n",
    "'''\n",
    "    df_university_lookup = pd.DataFrame([['UC', 'University of California'], \n",
    "              ['penn', 'University of Pennsylvania'],\n",
    "              ['NTU', 'Nanyang Technological University'],\n",
    "                                    ['Nanyang Technological University','Nanyang Technological University'],\n",
    "                                    ['University of Pennsylvania','University of Pennsylvania'],\n",
    "                                    ['University of California','University of California']])\n",
    "\n",
    "    df_university_lookup.columns = ['University_shortName', 'UniversityName']\n",
    "\n",
    "# To populate Country in a separate columns as data enrcihment, creating dataframe for university and their offering country\n",
    "#Ideally separate reference table should be created for university and offering country, as we have limited university, this will help later to extend the code\n",
    "    df_uni_country_lookup = pd.DataFrame([['USA', 'University of California'], \n",
    "              ['USA', 'University of Pennsylvania'],\n",
    "              ['Singapore', 'Nanyang Technological University'],['Singapore','Singapore Management University']])\n",
    "    df_uni_country_lookup.columns = ['Offering_country','Offering_university']\n",
    "\n",
    "\n",
    "# Joining df_file and df_university_lookup as left join to handle below situation and later will drop additional columnn as part of join.\n",
    "#UC and University of California\n",
    "#Basically we will use the df_university_lookup['UniversityName'] for University name\n",
    "    merged_df = pd.merge(df_file, df_university_lookup, \n",
    "                     left_on = 'UniversityName', \n",
    "                     right_on = 'University_shortName', \n",
    "                     how='left')\n",
    "\n",
    "# Joining above merged_df and df_uni_country_lookup to populate the offering country for each university as part of data enrichment\n",
    "    df = pd.merge(merged_df, df_uni_country_lookup, \n",
    "                     left_on = 'UniversityName_y', \n",
    "                     right_on = 'Offering_university', \n",
    "                     how='left')\n",
    "# Dropping additional columns which got populate becuase of above joining condition\n",
    "    df.drop(['UniversityName_x','University_shortName','UniversityName_y'], axis =1)\n",
    "\n",
    "\n",
    "# Dropping Duplicate CourseId\n",
    "    df_aqilliz_assessement = df.drop_duplicates(subset=['CourseID'], keep=False)\n",
    "\n",
    "# Checking and filtering out if Courseid length is 8 digit and all numeric\n",
    "    df_aqilliz_assessement = df_aqilliz_assessement[\n",
    "                       df_aqilliz_assessement['CourseID'].apply(lambda x: len(x)==8) &\n",
    "                       df_aqilliz_assessement['CourseID'].str.isnumeric()\n",
    "                    ]\n",
    "\n",
    "# Checking if CourseId starting as 1111 and populating CourseType as 'Certificate' else 'Diploma'\n",
    "    df_aqilliz_assessement['CourseType'] = df_aqilliz_assessement['CourseID'].apply(lambda x: 'Certificate' if x.startswith('1111') else 'Diploma')\n",
    "\n",
    "#Populating Metadata Column for auditing purpose\n",
    "    df_aqilliz_assessement['Processed_Date'] = pd.datetime.now().strftime(\"%d/%m/%Y %I:%M:%S\") # Processing Date as sysdate\n",
    "    df_aqilliz_assessement['Processed_By'] = 'ETL_USER' # which ETL job has populated the data\n",
    "    df_aqilliz_assessement['RowNumber'] = np.arange(len(df_aqilliz_assessement)) # Populated the row number and later can be used to check the count in case of batch job on a periodic manner\n",
    "\n",
    "\n",
    "# To route the bad data into separate dataframe and reporting purpose\n",
    "# Checking If Courseid is duplicated , length is not 8 and its not fully numeric and loading into dataframe- DF_Duplicated_CourseId\n",
    "    DF_Duplicated_CourseId = df[df.duplicated(['CourseID'],keep=False) |\n",
    "                            df['CourseID'].apply(lambda x: len(x)!=8) |\n",
    "                            df['CourseID'].str.isalpha()]\n",
    "\n",
    "#Populating Metadata Column for auditing purpose for bad data\n",
    "    DF_Duplicated_CourseId['Processed_Date'] = pd.datetime.now().strftime(\"%d/%m/%Y %I:%M:%S\")\n",
    "    DF_Duplicated_CourseId['Processed_By'] = 'ETL_USER'\n",
    "    DF_Duplicated_CourseId['RowNumber'] = np.arange(len(DF_Duplicated_CourseId))\n",
    "\n",
    "# Dropping additional column, since we checked the data quality issue on the original dataframe\n",
    "    DF_Duplicated_CourseId.drop(['UniversityName_x','University_shortName','UniversityName_y'], axis =1)\n",
    "\n",
    "# loading the cleaned data and bad data into separate directory\n",
    "# Cleaned data is into dataframe 'df_aqilliz_assessement'\n",
    "# Bad data isinto dataframe 'DF_Duplicated_CourseId'\n",
    "    Cleaned_data = df_aqilliz_assessement.to_csv (r'/Users/chaitanyapandey/Documents/Aqilliz/Cleaned_directory\\export_Cleaned_data.csv', index = None, header=True)\n",
    "    bad_Data = DF_Duplicated_CourseId.to_csv (r'/Users/chaitanyapandey/Documents/Aqilliz/Bad_directory\\export_Bad_data.csv', index = None, header=True)\n",
    "\n",
    "\n",
    "'''\n",
    "Task 3 : Data loading\n",
    "loading cleaned data 'df_aqilliz_assessement' into mySql database\n",
    "\n",
    "Designed Database Schema created is \n",
    "\n",
    "CREATE DATABASE TestDatabase;\n",
    "GO\n",
    "\n",
    "USE TestDatabase;\n",
    "CREATE TABLE df_aqilliz_assessement ( \n",
    "   Course_name Varchar(500) NOT NULL,\n",
    "   CourseID Int NOT NULL,\n",
    "   CourseName Varchar(30) NOT NULL,\n",
    "   Year_mnth Varchar(10),\n",
    "   Offering_country Varchar(50),\n",
    "   Offering_university Varchar(200),\n",
    "   CourseType Varchar(100),\n",
    "   Processed_Date Timestamp,\n",
    "   Processed_By Varchar(50),\n",
    "   RowNumber Int\n",
    "     );\n",
    "'''\n",
    "\n",
    "    user = 'root'\n",
    "    passw = 'root123'\n",
    "    host =  'local'\n",
    "    port = 3306\n",
    "    database = 'TestDatabase'\n",
    "\n",
    "\n",
    "    conn = pymysql.connect(host=host,\n",
    "                       port=port,\n",
    "                       user=user, \n",
    "                       passwd=passw,  \n",
    "                       db=database,\n",
    "                       charset='utf8')\n",
    "\n",
    "    df_aqilliz_assessement.to_sql(name=database, con=conn, if_exists = 'append', index=False, flavor = 'mysql')\n",
    "\n",
    "\n",
    "if __name__ = '__main__':\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
